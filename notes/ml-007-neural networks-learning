ml-007-neural networks-learning

L = total layers in network
s(l) = number of units in layer l

If K classes of output units
- Binary classificaton K = 1,2
- Multi-class classification, K >=3

Total Cost function:
J(θ) = (-1/m)*sum(sum(y(k)*log(hθ(x(k))) + (1-y(k))log(1-hθ(x(k)))))

Backpropagation
tracks "errors" between layers.
Training set {(x(1),y(1),...,(x(m),y(m))}
Set DELTA(l)(ij) = 0 for all l,i,j
For i = 1  to m
Set a(1) = x(i) -> input layer
Perform forward propagation to compute a(l) for l = 2, 3,..., L
Using y(i), compute delta(L) = a(L) - y(i)
Compute delta(L-1), delta(L-2),..., delta(2)
DELTA(l) := DELTA(l) + delta(l+1)*(a(l)')
D(l)(ij) = (1/m)*DELTA(l)(ij) + λ*θ(l)(ij) if j != 0
D(l)(ij) = (1/m)*DELTA(l)(ij) if j = 0

Training set {(x(1),y(1),...,(x(2),y(2))}
FP using x(1) followed by BP using y(1). Then FP using x(2) followed by BP using y(2).

error of cost for a(l)(j) (unit j in layer l)

matrix unrolling to vector:
A(:)
to rebuild use reshape

Gradient Checking
Used to validate BackPropagation
use epsilon ~= 10(-4)
(((x+e)^3) - ((x-e)^3))/(2*e)

for i =  1:n
thetaPlus = theta;
thetaPlus(i) = thetaPlus(i) + EPSILON;
thetaMinus = theta;
thetaMinus(i) = thetaMinus(i) - EPSILON;
gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*EPSILON);
end
Check that gradApprox ~= DVec

Implementation Notes:
- Implement BackPropagation to compute DVec (unrolled)
- Implement numerical gradient check to compute gradApprox
- Make sure they give similar values
- TURN OFF gradient checking - VERY EXPENSIVE

Random Initialzation since Zero Initialization does not work for neural networks and really leads to learning only one feature
- Need symetry breaking
- for each Theta1 = rand(10,11)*(2*INIT_EPSILON) - INIT_EPSILON

Putting it all together:
- Pick a network architecture (input, hidden, output)
- Number of input units == dimension of features (x(i))
- Number of output units == number of classes in vector form
- Default: 1 hidden layer of if >1 have some number of units per hidden layer (more units the better)
- Training a nerual network:
  - Randomly initialize weights
  - for i = 1 : m
  - Implement forward propagation to get  hθ(x(i)) for any x(i)
  - Implement cost function J(θ)
  - Implement backward propagation to complute partial derivatives of J(θ)(l), ignore bias unit
  - Use gradient checking to validate partial derivatives
  - Disable gradient checking code
  - Use gradient dedcent or advance optimization method with backpropagate to try to minimize J(θ)
******Potential of getting stuck on local minima



